{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b07264-3133-4d2b-ad88-8709fbf92ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing dataset: NHM-beetles-crops\n",
      "==============================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m img_w, img_h \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Get SAM3 internal sizes\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m sam_input_w, sam_input_h \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m     \u001b[38;5;66;03m# actual size SAM3 used\u001b[39;00m\n\u001b[1;32m     93\u001b[0m orig_w, orig_h \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]            \u001b[38;5;66;03m# original before SAM3\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Compute scaling correctly\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_size'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "import numpy as np\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "device = \"cuda:0\"\n",
    "bpe_path = \"./assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "\n",
    "root_dataset = \"./flatbug-dataset\"\n",
    "\n",
    "allowed_folders = {\n",
    "    \"cao2022\",\n",
    "    \"sittinger2023\",\n",
    "    \"amarathunga2022\",\n",
    "    \"biodiscover-arm\",\n",
    "}\n",
    "\n",
    "prompt_text = \"insects\"\n",
    "max_size = 640\n",
    "category_id = 1\n",
    "\n",
    "def mask_to_polygon(mask_np):\n",
    "    import cv2\n",
    "    mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    for cnt in contours:\n",
    "        if len(cnt) >= 3:\n",
    "            polygons.append(cnt.reshape(-1).tolist())\n",
    "    return polygons\n",
    "\n",
    "# ==========================\n",
    "# INIT SAM3 MODEL\n",
    "# ==========================\n",
    "model = build_sam3_image_model(bpe_path=bpe_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "processor = Sam3Processor(model, device=device, confidence_threshold=0.5)\n",
    "\n",
    "# ==========================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for dataset_name in sorted(os.listdir(root_dataset)):\n",
    "\n",
    "    if dataset_name not in allowed_folders:\n",
    "        continue\n",
    "\n",
    "    dataset_path = os.path.join(root_dataset, dataset_name)\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # *** SAVE OUTPUTS PER DATASET ***\n",
    "    output_json = os.path.join(dataset_path, f\"sam3_results_BB.json\")\n",
    "    output_image_folder = os.path.join(dataset_path, \"sam3_output_images\")\n",
    "\n",
    "    os.makedirs(output_image_folder, exist_ok=True)\n",
    "\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": category_id, \"name\": prompt_text}]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    image_id = 1\n",
    "\n",
    "    # Loop images in this dataset folder\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(dataset_path, filename)\n",
    "\n",
    "        orig_image = Image.open(img_path).convert(\"RGB\")\n",
    "        orig_w, orig_h = orig_image.size\n",
    "        draw_image = orig_image.copy()\n",
    "        draw = ImageDraw.Draw(draw_image)\n",
    "\n",
    "        # Resize for SAM3\n",
    "        image = orig_image.copy()\n",
    "        image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "        img_w, img_h = image.size\n",
    "\n",
    "        # Get SAM3 internal sizes\n",
    "        sam_input_w, sam_input_h = state[\"image_size\"]     # actual size SAM3 used\n",
    "        orig_w, orig_h = state[\"original_size\"]            # original before SAM3\n",
    "\n",
    "          \n",
    "        # Compute scaling correctly\n",
    "        scale_x = orig_w / sam_input_w\n",
    "        scale_y = orig_h / sam_input_h\n",
    "\n",
    "        print(f\"  - {filename}: original=({orig_w},{orig_h}), resized=({img_w},{img_h})\")\n",
    "\n",
    "        # SAM3 inference\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            with torch.inference_mode():\n",
    "                state = processor.set_image(image)\n",
    "                processor.reset_all_prompts(state)\n",
    "                state = processor.set_text_prompt(prompt_text, state)\n",
    "\n",
    "        masks = state.get(\"masks\", [])\n",
    "        boxes = state.get(\"boxes\", [])\n",
    "        scores = state.get(\"scores\", [])\n",
    "\n",
    "        coco_output[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": filename,\n",
    "            \"width\": orig_w,\n",
    "            \"height\": orig_h\n",
    "        })\n",
    "\n",
    "        # Process detected masks\n",
    "        for idx, mask in enumerate(masks):\n",
    "\n",
    "            # Rescale BBox\n",
    "            x0, y0, x1, y1 = boxes[idx].detach().cpu().tolist()\n",
    "            x0 *= scale_x\n",
    "            y0 *= scale_y\n",
    "            x1 *= scale_x\n",
    "            y1 *= scale_y\n",
    "            w = x1 - x0\n",
    "            h = y1 - y0\n",
    "\n",
    "            # Draw on image\n",
    "            draw.rectangle([x0, y0, x0 + w, y0 + h], outline=\"red\", width=2)\n",
    "            draw.text((x0, max(0, y0 - 10)), f\"{scores[idx]:.2f}\", fill=\"red\")\n",
    "\n",
    "            # Resize mask to original\n",
    "            mask_resized = np.array(\n",
    "                Image.fromarray(mask_np.astype(np.uint8)*255).resize((orig_w, orig_h), Image.NEAREST)\n",
    "            ) > 127\n",
    "            mask_resized = mask_resized.astype(np.uint8)\n",
    "\n",
    "            seg = mask_to_polygon(mask_resized)\n",
    "            if not seg:\n",
    "                continue\n",
    "\n",
    "            coco_output[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [float(x0), float(y0), float(w), float(h)],\n",
    "                \"segmentation\": seg,\n",
    "                \"area\": float(np.sum(mask_resized)),\n",
    "                \"iscrowd\": 0,\n",
    "                \"score\": float(scores[idx])\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "        # Save visualization image\n",
    "        out_img_path = os.path.join(output_image_folder, filename)\n",
    "        draw_image.save(out_img_path)\n",
    "\n",
    "        # Cleanup\n",
    "        del state, masks, boxes, scores\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "    # Save JSON\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "\n",
    "    print(f\"Saved SAM3 JSON → {output_json}\")\n",
    "    print(f\"Saved bounding-box images → {output_image_folder}\")\n",
    "\n",
    "print(\"\\nAll datasets processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b4867e-4541-4f5f-8311-766e08dadd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing dataset: NHM-beetles-crops\n",
      "==============================\n",
      "Saved SAM3 JSON → ./flatbug-dataset/NHM-beetles-crops/sam3_results_BB.json\n",
      "Saved bounding-box images → ./flatbug-dataset/NHM-beetles-crops/sam3_output_images\n",
      "\n",
      "All datasets processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "device = \"cuda:0\"\n",
    "bpe_path = \"./assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "\n",
    "root_dataset = \"./flatbug-dataset\"\n",
    "\n",
    "allowed_folders = {\n",
    "    \"NHM-beetles-crops\",\n",
    "}\n",
    "\n",
    "prompt_text = \"insects\"\n",
    "category_id = 1\n",
    "\n",
    "def mask_to_polygon(mask_np):\n",
    "    import cv2\n",
    "    mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    for cnt in contours:\n",
    "        if len(cnt) >= 3:\n",
    "            polygons.append(cnt.reshape(-1).tolist())\n",
    "    return polygons\n",
    "\n",
    "# ==========================\n",
    "# INIT SAM3 MODEL\n",
    "# ==========================\n",
    "model = build_sam3_image_model(bpe_path=bpe_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "processor = Sam3Processor(model, device=device, confidence_threshold=0.5)\n",
    "\n",
    "# ==========================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for dataset_name in sorted(os.listdir(root_dataset)):\n",
    "\n",
    "    if dataset_name not in allowed_folders:\n",
    "        continue\n",
    "\n",
    "    dataset_path = os.path.join(root_dataset, dataset_name)\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    output_json = os.path.join(dataset_path, \"sam3_results_BB.json\")\n",
    "    output_image_folder = os.path.join(dataset_path, \"sam3_output_images\")\n",
    "    os.makedirs(output_image_folder, exist_ok=True)\n",
    "\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": category_id, \"name\": prompt_text}]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    image_id = 1\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(dataset_path, filename)\n",
    "        orig_image = Image.open(img_path).convert(\"RGB\")\n",
    "        orig_w, orig_h = orig_image.size\n",
    "        draw_image = orig_image.copy()\n",
    "        draw = ImageDraw.Draw(draw_image)\n",
    "\n",
    "        # ===== SAM3 PREDICTION =====\n",
    "        image = orig_image.copy()  # no manual resizing\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            with torch.inference_mode():\n",
    "                state = processor.set_image(image)\n",
    "                processor.reset_all_prompts(state)\n",
    "                state = processor.set_text_prompt(prompt_text, state)\n",
    "\n",
    "        masks = state[\"masks\"]\n",
    "        boxes = state[\"boxes\"]\n",
    "        scores = state[\"scores\"]\n",
    "\n",
    "        # If at least one mask exists, use it to get internal SAM3 size\n",
    "        if len(masks) > 0:\n",
    "            mask_np = masks[0].cpu().numpy().squeeze()\n",
    "            sam_h, sam_w = mask_np.shape\n",
    "        else:\n",
    "            sam_w, sam_h = orig_w, orig_h  # fallback if no masks\n",
    "        scale_x = orig_w / sam_w\n",
    "        scale_y = orig_h / sam_h\n",
    "\n",
    "        coco_output[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": filename,\n",
    "            \"width\": orig_w,\n",
    "            \"height\": orig_h\n",
    "        })\n",
    "\n",
    "        for idx, mask in enumerate(masks):\n",
    "            # Scale bounding box\n",
    "            x0, y0, x1, y1 = boxes[idx].cpu().tolist()\n",
    "            x0 *= scale_x\n",
    "            y0 *= scale_y\n",
    "            x1 *= scale_x\n",
    "            y1 *= scale_y\n",
    "            w = x1 - x0\n",
    "            h = y1 - y0\n",
    "\n",
    "            # Draw bounding box\n",
    "            draw.rectangle([x0, y0, x0 + w, y0 + h], outline=\"red\", width=2)\n",
    "            draw.text((x0, max(0, y0 - 10)), f\"{scores[idx]:.2f}\", fill=\"red\")\n",
    "\n",
    "            # Resize mask to original size\n",
    "            mask_np = mask.cpu().numpy().squeeze()\n",
    "            mask_resized = np.array(\n",
    "                Image.fromarray((mask_np * 255).astype(np.uint8)).resize((orig_w, orig_h), Image.NEAREST)\n",
    "            ) > 127\n",
    "            mask_resized = mask_resized.astype(np.uint8)\n",
    "\n",
    "            seg = mask_to_polygon(mask_resized)\n",
    "            if not seg:\n",
    "                continue\n",
    "\n",
    "            coco_output[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [float(x0), float(y0), float(w), float(h)],\n",
    "                \"segmentation\": seg,\n",
    "                \"area\": float(np.sum(mask_resized)),\n",
    "                \"iscrowd\": 0,\n",
    "                \"score\": float(scores[idx])\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "        # Save visualization image\n",
    "        out_img_path = os.path.join(output_image_folder, filename)\n",
    "        draw_image.save(out_img_path)\n",
    "\n",
    "        # Cleanup\n",
    "        del state, masks, boxes, scores\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "    # Save JSON\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "\n",
    "    print(f\"Saved SAM3 JSON → {output_json}\")\n",
    "    print(f\"Saved bounding-box images → {output_image_folder}\")\n",
    "\n",
    "print(\"\\nAll datasets processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23fe5473-4b97-4393-85c5-25a175474a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing dataset: amarathunga2022\n",
      "==============================\n",
      "Saved SAM3 JSON → ./flatbug-dataset/amarathunga2022/sam3_results.json\n",
      "Saved bounding-box images → ./flatbug-dataset/amarathunga2022/sam3_output_images\n",
      "\n",
      "==============================\n",
      "Processing dataset: biodiscover-arm\n",
      "==============================\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.60 GiB. GPU 0 has a total capacity of 11.90 GiB of which 986.62 MiB is free. Process 439745 has 10.93 GiB memory in use. Of the allocated memory 7.50 GiB is allocated by PyTorch, and 3.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 92\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m         processor\u001b[38;5;241m.\u001b[39mreset_all_prompts(state)\n\u001b[1;32m     94\u001b[0m         state \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mset_text_prompt(prompt_text, state)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/sam3/sam3/model/sam3_image_processor.py:59\u001b[0m, in \u001b[0;36mSam3Processor.set_image\u001b[0;34m(self, image, state)\u001b[0m\n\u001b[1;32m     57\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_height\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m height\n\u001b[1;32m     58\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_width\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m width\n\u001b[0;32m---> 59\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone_out\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m inst_interactivity_en \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minst_interactive_predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inst_interactivity_en \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msam2_backbone_out\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone_out\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/data/sam3/sam3/model/vl_combiner.py:79\u001b[0m, in \u001b[0;36mSAM3VLBackbone.forward_image\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactivation_ckpt_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_image_no_act_ckpt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact_ckpt_enable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_ckpt_whole_vision_backbone\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/sam3/sam3/model/act_ckpt_utils.py:86\u001b[0m, in \u001b[0;36mactivation_ckpt_wrapper.<locals>.act_ckpt_wrapper\u001b[0;34m(act_ckpt_enable, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     ret \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m     83\u001b[0m         module, \u001b[38;5;241m*\u001b[39margs, use_reentrant\u001b[38;5;241m=\u001b[39muse_reentrant, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/data/sam3/sam3/model/vl_combiner.py:86\u001b[0m, in \u001b[0;36mSAM3VLBackbone._forward_image_no_act_ckpt\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_image_no_act_ckpt\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Forward through backbone\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     sam3_features, sam3_pos, sam2_features, sam2_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_backbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43msamples\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# Discard the lowest resolution features\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         sam3_features, sam3_pos \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m             sam3_features[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp],\n\u001b[1;32m     93\u001b[0m             sam3_pos[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp],\n\u001b[1;32m     94\u001b[0m         )\n",
      "File \u001b[0;32m/data/sam3/sam3/model/necks.py:108\u001b[0m, in \u001b[0;36mSam3DualViTDetNeck.forward\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensor_list: List[torch\u001b[38;5;241m.\u001b[39mTensor]\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     Optional[List[torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    107\u001b[0m ]:\n\u001b[0;32m--> 108\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     sam3_out, sam3_pos \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    110\u001b[0m     sam2_out, sam2_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/sam3/sam3/model/vitdet.py:840\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    838\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(blk, x, use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 840\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_attn_ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_interm_layers \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_attn_ids\n\u001b[1;32m    843\u001b[0m ):\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_attn_ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/sam3/sam3/model/vitdet.py:605\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    602\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    603\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 605\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/sam3/sam3/model/vitdet.py:502\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    499\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    500\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 502\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    505\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    506\u001b[0m         x\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    509\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.60 GiB. GPU 0 has a total capacity of 11.90 GiB of which 986.62 MiB is free. Process 439745 has 10.93 GiB memory in use. Of the allocated memory 7.50 GiB is allocated by PyTorch, and 3.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "device = \"cuda:0\"\n",
    "bpe_path = \"./assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "\n",
    "root_dataset = \"./flatbug-dataset\"\n",
    "\n",
    "allowed_folders = {\n",
    "    \"alus\",\n",
    "    \"bioscan\",\n",
    "    \"diversityscanner\",\n",
    "    #\"nhm-beetles-crops\",\n",
    "    \"artaxor\",\n",
    "    \"collembolai\",\n",
    "    \"gernat2018\",\n",
    "    \"cao2022\",\n",
    "    \"sittinger2023\",\n",
    "    #\"amarathunga2022\",\n",
    "    \"biodiscover-arm\",\n",
    "}\n",
    "\n",
    "prompt_text = \"whole insects only; ignore partial or incomplete insect parts\"\n",
    "category_id = 1\n",
    "\n",
    "def mask_to_polygon(mask_np):\n",
    "    import cv2\n",
    "    mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    for cnt in contours:\n",
    "        if len(cnt) >= 3:\n",
    "            polygons.append(cnt.reshape(-1).tolist())\n",
    "    return polygons\n",
    "\n",
    "# ==========================\n",
    "# INIT SAM3 MODEL\n",
    "# ==========================\n",
    "model = build_sam3_image_model(bpe_path=bpe_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "processor = Sam3Processor(model, device=device, confidence_threshold=0.5)\n",
    "\n",
    "# ==========================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for dataset_name in sorted(os.listdir(root_dataset)):\n",
    "    if dataset_name not in allowed_folders:\n",
    "        continue\n",
    "\n",
    "    dataset_path = os.path.join(root_dataset, dataset_name)\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    output_json = os.path.join(dataset_path, \"sam3_results.json\")\n",
    "    output_image_folder = os.path.join(dataset_path, \"sam3_output_images\")\n",
    "    os.makedirs(output_image_folder, exist_ok=True)\n",
    "\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": category_id, \"name\": prompt_text}]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    image_id = 1\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(dataset_path, filename)\n",
    "        orig_image = Image.open(img_path).convert(\"RGB\")\n",
    "        orig_w, orig_h = orig_image.size\n",
    "        draw_image = orig_image.copy()\n",
    "        draw = ImageDraw.Draw(draw_image)\n",
    "\n",
    "        # ===== SAM3 PREDICTION =====\n",
    "        image = orig_image.copy()  # no manual resizing\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            with torch.inference_mode():\n",
    "                state = processor.set_image(image)\n",
    "                processor.reset_all_prompts(state)\n",
    "                state = processor.set_text_prompt(prompt_text, state)\n",
    "\n",
    "        masks = state[\"masks\"]\n",
    "        boxes = state[\"boxes\"]\n",
    "        scores = state[\"scores\"]\n",
    "\n",
    "        # Determine SAM3 internal size from mask shape\n",
    "        if len(masks) > 0:\n",
    "            sam_h, sam_w = masks.shape[-2:]\n",
    "        else:\n",
    "            sam_w, sam_h = orig_w, orig_h\n",
    "        scale_x = orig_w / sam_w\n",
    "        scale_y = orig_h / sam_h\n",
    "\n",
    "        # Add image info\n",
    "        coco_output[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": filename,\n",
    "            \"width\": orig_w,\n",
    "            \"height\": orig_h\n",
    "        })\n",
    "\n",
    "        for idx, mask in enumerate(masks):\n",
    "            x0, y0, x1, y1 = boxes[idx].cpu().tolist()\n",
    "            x0 *= scale_x\n",
    "            y0 *= scale_y\n",
    "            x1 *= scale_x\n",
    "            y1 *= scale_y\n",
    "            w = x1 - x0\n",
    "            h = y1 - y0\n",
    "\n",
    "            # Draw BB\n",
    "            draw.rectangle([x0, y0, x0 + w, y0 + h], outline=\"red\", width=2)\n",
    "            draw.text((x0, max(0, y0 - 10)), f\"{scores[idx]:.2f}\", fill=\"red\")\n",
    "\n",
    "            # Resize mask\n",
    "            mask_np = mask.cpu().numpy().squeeze()\n",
    "            mask_resized = np.array(\n",
    "                Image.fromarray((mask_np * 255).astype(np.uint8))\n",
    "                .resize((orig_w, orig_h), Image.NEAREST)\n",
    "            ) > 127\n",
    "            mask_resized = mask_resized.astype(np.uint8)\n",
    "\n",
    "            seg = mask_to_polygon(mask_resized)\n",
    "            if not seg:\n",
    "                continue\n",
    "\n",
    "            coco_output[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"file_name\": filename,          # <-- Added file_name here\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [float(x0), float(y0), float(w), float(h)],\n",
    "                \"segmentation\": seg,\n",
    "                \"area\": float(np.sum(mask_resized)),\n",
    "                \"iscrowd\": 0,\n",
    "                \"score\": float(scores[idx])\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "        # Save visualization\n",
    "        out_img_path = os.path.join(output_image_folder, filename)\n",
    "        draw_image.save(out_img_path)\n",
    "\n",
    "        # Cleanup\n",
    "        del state, masks, boxes, scores\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "    # Save JSON\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "\n",
    "    print(f\"Saved SAM3 JSON → {output_json}\")\n",
    "    print(f\"Saved bounding-box images → {output_image_folder}\")\n",
    "\n",
    "print(\"\\nAll datasets processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85783b-3c2f-42cf-9e88-09fce0ebfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## kill the terminal to restart kernal and clear the memory\n",
    "!pkill -f ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77f883d-c1d6-4db5-b99f-77c99525720f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 12 23:00:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN Xp                Off |   00000000:03:00.0 Off |                  N/A |\n",
      "| 71%   87C    P2            234W /  250W |   11789MiB /  12288MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA TITAN Xp                Off |   00000000:83:00.0 Off |                  N/A |\n",
      "| 23%   27C    P8              9W /  250W |       5MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d44b89-1c48-4703-b393-1e4e30ecdd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing dataset: cao2022\n",
      "==============================\n",
      "Saved SAM3 JSON → ./flatbug-dataset/cao2022/sam3_results.json\n",
      "Saved SAM3 images → ./flatbug-dataset/cao2022/sam3_output_images\n",
      "\n",
      "All datasets processed successfully.\n"
     ]
    }
   ],
   "source": [
    "## BB with mask overlay\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageColor\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "device = \"cuda:0\"\n",
    "bpe_path = \"./assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "\n",
    "root_dataset = \"./flatbug-dataset\"\n",
    "\n",
    "allowed_folders = {\n",
    "    # \"ALUS\",\n",
    "    # \"BIOSCAN\",\n",
    "    # \"DiversityScanner\",\n",
    "    # \"nhm-beetles-crops\",\n",
    "    # \"ArTaxOr\",\n",
    "    # \"CollembolAI\",\n",
    "    # \"gernat2018\",\n",
    "     \"cao2022\",\n",
    "    # \"sittinger2023\",\n",
    "    # \"amarathunga2022\",\n",
    "    # \"biodiscover-arm\",\n",
    "}\n",
    "\n",
    "prompt_text = \"insects\"\n",
    "category_id = 1\n",
    "\n",
    "def mask_to_polygon(mask_np):\n",
    "    import cv2\n",
    "    mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    polygons = []\n",
    "    for cnt in contours:\n",
    "        if len(cnt) >= 3:\n",
    "            polygons.append(cnt.reshape(-1).tolist())\n",
    "    return polygons\n",
    "\n",
    "# ==========================\n",
    "# INIT SAM3 MODEL\n",
    "# ==========================\n",
    "model = build_sam3_image_model(bpe_path=bpe_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "processor = Sam3Processor(model, device=device, confidence_threshold=0.5)\n",
    "\n",
    "# ==========================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ==========================\n",
    "for dataset_name in sorted(os.listdir(root_dataset)):\n",
    "    if dataset_name not in allowed_folders:\n",
    "        continue\n",
    "\n",
    "    dataset_path = os.path.join(root_dataset, dataset_name)\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    output_json = os.path.join(dataset_path, \"sam3_results.json\")\n",
    "    output_image_folder = os.path.join(dataset_path, \"sam3_output_images\")\n",
    "    os.makedirs(output_image_folder, exist_ok=True)\n",
    "\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": category_id, \"name\": prompt_text}]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    image_id = 1\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(dataset_path, filename)\n",
    "        orig_image = Image.open(img_path).convert(\"RGB\")\n",
    "        orig_w, orig_h = orig_image.size\n",
    "\n",
    "        # visualization image (RGB)\n",
    "        draw_image = orig_image.copy()\n",
    "        draw = ImageDraw.Draw(draw_image)\n",
    "\n",
    "        # new overlay image (RGBA) for masks\n",
    "        overlay = Image.new(\"RGBA\", (orig_w, orig_h), (0, 0, 0, 0))\n",
    "        overlay_draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "        # ===== SAM3 PREDICTION =====\n",
    "        image = orig_image.copy()\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            with torch.inference_mode():\n",
    "                state = processor.set_image(image)\n",
    "                processor.reset_all_prompts(state)\n",
    "                state = processor.set_text_prompt(prompt_text, state)\n",
    "\n",
    "        masks = state[\"masks\"]\n",
    "        boxes = state[\"boxes\"]\n",
    "        scores = state[\"scores\"]\n",
    "\n",
    "        # Determine SAM3 internal size\n",
    "        if len(masks) > 0:\n",
    "            sam_h, sam_w = masks.shape[-2:]\n",
    "        else:\n",
    "            sam_w, sam_h = orig_w, orig_h\n",
    "        scale_x = orig_w / sam_w\n",
    "        scale_y = orig_h / sam_h\n",
    "\n",
    "        # Add image info\n",
    "        coco_output[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": filename,\n",
    "            \"width\": orig_w,\n",
    "            \"height\": orig_h\n",
    "        })\n",
    "\n",
    "        for idx, mask in enumerate(masks):\n",
    "\n",
    "            # Scale BB\n",
    "            x0, y0, x1, y1 = boxes[idx].cpu().tolist()\n",
    "            x0 *= scale_x;  y0 *= scale_y\n",
    "            x1 *= scale_x;  y1 *= scale_y\n",
    "            w = x1 - x0\n",
    "            h = y1 - y0\n",
    "\n",
    "            # Draw BB\n",
    "            draw.rectangle([x0, y0, x0 + w, y0 + h], outline=\"red\", width=2)\n",
    "            draw.text((x0, max(0, y0 - 12)), f\"{scores[idx]:.2f}\", fill=\"red\")\n",
    "\n",
    "            # Resize mask to original size\n",
    "            mask_np = mask.cpu().numpy().squeeze()\n",
    "            mask_resized = np.array(\n",
    "                Image.fromarray((mask_np * 255).astype(np.uint8))\n",
    "                .resize((orig_w, orig_h), Image.NEAREST)\n",
    "            ) > 127\n",
    "            mask_resized = mask_resized.astype(np.uint8)\n",
    "\n",
    "            # ==========================\n",
    "            # DRAW MASK OVERLAY\n",
    "            # ==========================\n",
    "            # transparent blue overlay (adjust alpha if needed)\n",
    "            mask_color = (30, 144, 255, 110)  # RGBA: semi-transparent blue\n",
    "            ys, xs = np.where(mask_resized == 1)\n",
    "            for x, y in zip(xs, ys):\n",
    "                overlay_draw.point((x, y), fill=mask_color)\n",
    "\n",
    "            seg = mask_to_polygon(mask_resized)\n",
    "            if not seg:\n",
    "                continue\n",
    "\n",
    "            coco_output[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"file_name\": filename,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [float(x0), float(y0), float(w), float(h)],\n",
    "                \"segmentation\": seg,\n",
    "                \"area\": float(np.sum(mask_resized)),\n",
    "                \"iscrowd\": 0,\n",
    "                \"score\": float(scores[idx])\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "        # =========================================\n",
    "        # COMPOSITE MASK OVERLAY + ORIGINAL IMAGE\n",
    "        # =========================================\n",
    "        draw_image = Image.alpha_composite(draw_image.convert(\"RGBA\"), overlay).convert(\"RGB\")\n",
    "\n",
    "        # Save visualization\n",
    "        out_img_path = os.path.join(output_image_folder, filename)\n",
    "        draw_image.save(out_img_path)\n",
    "\n",
    "        # Cleanup\n",
    "        del state, masks, boxes, scores\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "    # Save JSON\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "\n",
    "    print(f\"Saved SAM3 JSON → {output_json}\")\n",
    "    print(f\"Saved SAM3 images → {output_image_folder}\")\n",
    "\n",
    "print(\"\\nAll datasets processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bee059-431c-4439-bf11-6776295a9fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sam3env)",
   "language": "python",
   "name": "sam3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
