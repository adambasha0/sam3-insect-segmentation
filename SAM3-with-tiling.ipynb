{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c97b87e-11dc-4568-83cc-96d1381be3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM3 Model...\n",
      "Model Loaded.\n",
      "\n",
      "==============================\n",
      "Processing: cao2022\n",
      "==============================\n",
      "  [1] 000001.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 70 raw -> 14 unique\n",
      "  [2] 000002.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 65 raw -> 14 unique\n",
      "  [3] 000003.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 61 raw -> 16 unique\n",
      "  [4] 000025.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 54 raw -> 12 unique\n",
      "  [5] 000026.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 57 raw -> 14 unique\n",
      "  [6] 000027.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 62 raw -> 14 unique\n",
      "  [7] 000028.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 59 raw -> 13 unique\n",
      "  [8] 000051.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 64 raw -> 18 unique\n",
      "  [9] 000053.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 56 raw -> 11 unique\n",
      "  [10] 000075.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 52 raw -> 13 unique\n",
      "  [11] 000077.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 63 raw -> 14 unique\n",
      "  [12] 000078.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 54 raw -> 10 unique\n",
      "  [13] 000100.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 57 raw -> 13 unique\n",
      "  [14] 000101.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 53 raw -> 13 unique\n",
      "  [15] 000102.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 56 raw -> 13 unique\n",
      "  [16] 000103.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 62 raw -> 12 unique\n",
      "  [17] 000125.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 61 raw -> 12 unique\n",
      "  [18] 000126.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 59 raw -> 15 unique\n",
      "  [19] 000127.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 68 raw -> 13 unique\n",
      "  [20] 000128.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 63 raw -> 13 unique\n",
      "  [21] 000151.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 60 raw -> 13 unique\n",
      "  [22] 000153.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 61 raw -> 13 unique\n",
      "  [23] 000175.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 49 raw -> 12 unique\n",
      "  [24] 000177.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 77 raw -> 15 unique\n",
      "  [25] 000178.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 64 raw -> 14 unique\n",
      "  [26] 000200.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 55 raw -> 13 unique\n",
      "  [27] 000201.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 58 raw -> 15 unique\n",
      "  [28] 000202.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 62 raw -> 16 unique\n",
      "  [29] 000203.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 54 raw -> 10 unique\n",
      "  [30] 000225.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 51 raw -> 12 unique\n",
      "  [31] 000226.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 60 raw -> 13 unique\n",
      "  [32] 000227.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 58 raw -> 14 unique\n",
      "  [33] 000228.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 74 raw -> 18 unique\n",
      "  [34] 000251.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 57 raw -> 14 unique\n",
      "  [35] 000253.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 56 raw -> 11 unique\n",
      "  [36] 000275.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 55 raw -> 13 unique\n",
      "  [37] 000277.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 58 raw -> 14 unique\n",
      "  [38] 000278.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 58 raw -> 12 unique\n",
      "  [39] 000300.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 56 raw -> 15 unique\n",
      "  [40] 000301.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 57 raw -> 16 unique\n",
      "  [41] 000302.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 63 raw -> 13 unique\n",
      "  [42] 000303.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 56 raw -> 13 unique\n",
      "  [43] 000325.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 56 raw -> 16 unique\n",
      "  [44] 000326.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 47 raw -> 12 unique\n",
      "  [45] 000327.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 65 raw -> 14 unique\n",
      "  [46] 000328.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 60 raw -> 15 unique\n",
      "  [47] 000351.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 43 raw -> 12 unique\n",
      "  [48] 000378.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 53 raw -> 12 unique\n",
      "  [49] 000428.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 54 raw -> 11 unique\n",
      "  [50] 000478.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 60 raw -> 16 unique\n",
      "  [51] 000528.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 60 raw -> 11 unique\n",
      "  [52] 000578.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 61 raw -> 14 unique\n",
      "  [53] 000628.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 59 raw -> 14 unique\n",
      "  [54] 000678.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 56 raw -> 12 unique\n",
      "  [55] 000728.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 67 raw -> 15 unique\n",
      "  [56] 000778.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 56 raw -> 13 unique\n",
      "  [57] 000828.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 68 raw -> 17 unique\n",
      "  [58] 000878.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 59 raw -> 15 unique\n",
      "  [59] 000928.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 78 raw -> 18 unique\n",
      "  [60] 000978.jpg (1920x1080)... Split into 7 views (1 Global + 6 Tiles)\n",
      "    > Detections: 69 raw -> 16 unique\n",
      "Saved JSON to ./flatbug-dataset/cao2022/sam3_results_tiled.json\n",
      "\n",
      "All processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gc\n",
    "from PIL import Image, ImageDraw\n",
    "from sam3.model_builder import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "\n",
    "# ==========================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================\n",
    "DEVICE = \"cuda:0\"\n",
    "BPE_PATH = \"./assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "ROOT_DATASET = \"./flatbug-dataset\"\n",
    "\n",
    "# Folders to process\n",
    "ALLOWED_FOLDERS = {\n",
    "    \"cao2022\",\n",
    "}\n",
    "\n",
    "PROMPT_TEXT = \"insects\"\n",
    "CATEGORY_ID = 1\n",
    "\n",
    "# --- HYPER-INFERENCE PARAMETERS ---\n",
    "TILE_SIZE = 1024        # 1024x1024 is optimal for SAM-based models\n",
    "TILE_OVERLAP = 0.25     # 25% overlap to ensure edge objects are captured\n",
    "IOU_THRESHOLD = 0.5     # Intersection over Union for NMS (Deduplication)\n",
    "CONF_THRESHOLD = 0.45   # Confidence score to keep a prediction\n",
    "\n",
    "# ==========================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def get_sliding_window_crops(image, tile_size, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Slices image into overlapping tiles.\n",
    "    Returns: List of dicts {'image': PIL.Image, 'x': int, 'y': int}\n",
    "    \"\"\"\n",
    "    w, h = image.size\n",
    "    stride = int(tile_size * (1 - overlap_ratio))\n",
    "    \n",
    "    crops = []\n",
    "    # If image is smaller than tile, just return original\n",
    "    if w <= tile_size and h <= tile_size:\n",
    "        return [{'image': image, 'x': 0, 'y': 0}]\n",
    "\n",
    "    for y in range(0, h, stride):\n",
    "        for x in range(0, w, stride):\n",
    "            # Calculate coordinates\n",
    "            x_end = min(x + tile_size, w)\n",
    "            y_end = min(y + tile_size, h)\n",
    "            \n",
    "            # Adjust start point to ensure we don't have a tiny slice at the edge\n",
    "            x_start = max(0, x_end - tile_size)\n",
    "            y_start = max(0, y_end - tile_size)\n",
    "            \n",
    "            crop = image.crop((x_start, y_start, x_end, y_end))\n",
    "            crops.append({'image': crop, 'x': x_start, 'y': y_start})\n",
    "            \n",
    "    return crops\n",
    "\n",
    "def mask_to_global_polygon(mask_np, x_off, y_off, scale_x, scale_y):\n",
    "    \"\"\"\n",
    "    Extracts polygons from a local mask and transforms them to global coordinates.\n",
    "    Memory efficient: does not create full-sized masks.\n",
    "    \"\"\"\n",
    "    # Ensure binary uint8\n",
    "    mask_uint8 = (mask_np * 255).astype(np.uint8)\n",
    "    \n",
    "    # Find contours on the small tile mask\n",
    "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    polygons = []\n",
    "    for cnt in contours:\n",
    "        if len(cnt) < 3: continue # Ignore noise\n",
    "            \n",
    "        # Reshape to (N, 2)\n",
    "        points = cnt.reshape(-1, 2).astype(np.float32)\n",
    "        \n",
    "        # Transform: Global = Local * Scale + Offset\n",
    "        points[:, 0] = points[:, 0] * scale_x + x_off\n",
    "        points[:, 1] = points[:, 1] * scale_y + y_off\n",
    "        \n",
    "        # Check bounds (clamp to 0)\n",
    "        points = np.maximum(points, 0)\n",
    "        \n",
    "        # Flatten for COCO format\n",
    "        polygons.append(points.flatten().tolist())\n",
    "        \n",
    "    return polygons\n",
    "\n",
    "# ==========================\n",
    "# 3. INITIALIZE MODEL\n",
    "# ==========================\n",
    "print(\"Loading SAM3 Model...\")\n",
    "model = build_sam3_image_model(bpe_path=BPE_PATH)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "processor = Sam3Processor(model, device=DEVICE, confidence_threshold=CONF_THRESHOLD)\n",
    "print(\"Model Loaded.\")\n",
    "\n",
    "# ==========================\n",
    "# 4. MAIN PIPELINE\n",
    "# ==========================\n",
    "for dataset_name in sorted(os.listdir(ROOT_DATASET)):\n",
    "    if dataset_name not in ALLOWED_FOLDERS:\n",
    "        continue\n",
    "\n",
    "    dataset_path = os.path.join(ROOT_DATASET, dataset_name)\n",
    "    print(f\"\\n==============================\\nProcessing: {dataset_name}\\n==============================\")\n",
    "\n",
    "    # Output paths\n",
    "    output_json = os.path.join(dataset_path, f\"sam3_results_tiled.json\")\n",
    "    output_img_dir = os.path.join(dataset_path, \"sam3_vis_tiled\")\n",
    "    os.makedirs(output_img_dir, exist_ok=True)\n",
    "\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": CATEGORY_ID, \"name\": PROMPT_TEXT}]\n",
    "    }\n",
    "\n",
    "    ann_id = 1\n",
    "    img_id = 1\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(dataset_path, filename)\n",
    "        \n",
    "        try:\n",
    "            orig_image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping corrupt image {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        orig_w, orig_h = orig_image.size\n",
    "        print(f\"  [{img_id}] {filename} ({orig_w}x{orig_h})\", end=\"... \")\n",
    "\n",
    "        # --- A. PREPARE INFERENCE BATCH (Global + Tiles) ---\n",
    "        inference_items = []\n",
    "\n",
    "        # 1. Global Context View (Resized to TILE_SIZE)\n",
    "        # This helps find large insects or clusters\n",
    "        global_resized = orig_image.copy()\n",
    "        global_resized.thumbnail((TILE_SIZE, TILE_SIZE), Image.Resampling.LANCZOS)\n",
    "        inference_items.append({\n",
    "            \"image\": global_resized,\n",
    "            \"x_off\": 0, \"y_off\": 0,\n",
    "            \"scale_x\": orig_w / global_resized.width,\n",
    "            \"scale_y\": orig_h / global_resized.height,\n",
    "            \"type\": \"global\"\n",
    "        })\n",
    "\n",
    "        # 2. Tiled Views (Full Resolution)\n",
    "        # Only if image is larger than tile size\n",
    "        if orig_w > TILE_SIZE or orig_h > TILE_SIZE:\n",
    "            tiles = get_sliding_window_crops(orig_image, TILE_SIZE, TILE_OVERLAP)\n",
    "            for t in tiles:\n",
    "                inference_items.append({\n",
    "                    \"image\": t['image'],\n",
    "                    \"x_off\": t['x'], \"y_off\": t['y'],\n",
    "                    \"scale_x\": 1.0, \"scale_y\": 1.0, # No scaling for tiles\n",
    "                    \"type\": \"tile\"\n",
    "                })\n",
    "\n",
    "        print(f\"Split into {len(inference_items)} views (1 Global + {len(inference_items)-1} Tiles)\")\n",
    "\n",
    "        # --- B. RUN INFERENCE ---\n",
    "        raw_boxes = []\n",
    "        raw_scores = []\n",
    "        raw_masks_info = [] # Store metadata to reconstruct masks later\n",
    "\n",
    "        for item in inference_items:\n",
    "            # Run SAM3\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                with torch.inference_mode():\n",
    "                    state = processor.set_image(item[\"image\"])\n",
    "                    processor.reset_all_prompts(state)\n",
    "                    state = processor.set_text_prompt(PROMPT_TEXT, state)\n",
    "\n",
    "            masks = state.get(\"masks\", [])\n",
    "            boxes = state.get(\"boxes\", [])\n",
    "            scores = state.get(\"scores\", [])\n",
    "\n",
    "            if len(boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            # Transform results to Global Coordinates\n",
    "            for i, box in enumerate(boxes):\n",
    "                x0, y0, x1, y1 = box.detach().cpu().tolist()\n",
    "                \n",
    "                # Apply scaling (for global view) and offset (for tiles)\n",
    "                gx0 = x0 * item[\"scale_x\"] + item[\"x_off\"]\n",
    "                gy0 = y0 * item[\"scale_y\"] + item[\"y_off\"]\n",
    "                gx1 = x1 * item[\"scale_x\"] + item[\"x_off\"]\n",
    "                gy1 = y1 * item[\"scale_y\"] + item[\"y_off\"]\n",
    "\n",
    "                raw_boxes.append([gx0, gy0, gx1, gy1])\n",
    "                raw_scores.append(scores[i].item())\n",
    "                \n",
    "                # Keep mask as small numpy array to save RAM\n",
    "                # We will process polygons only for survivors\n",
    "                raw_masks_info.append({\n",
    "                    \"mask\": masks[i].detach().cpu().numpy().squeeze(),\n",
    "                    \"x_off\": item[\"x_off\"],\n",
    "                    \"y_off\": item[\"y_off\"],\n",
    "                    \"scale_x\": item[\"scale_x\"],\n",
    "                    \"scale_y\": item[\"scale_y\"]\n",
    "                })\n",
    "\n",
    "            # Clear VRAM after every tile\n",
    "            del state, masks, boxes, scores\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # --- C. DEDUPLICATION (NMS) ---\n",
    "        final_indices = []\n",
    "        if len(raw_boxes) > 0:\n",
    "            boxes_t = torch.tensor(raw_boxes, dtype=torch.float32).to(DEVICE)\n",
    "            scores_t = torch.tensor(raw_scores, dtype=torch.float32).to(DEVICE)\n",
    "            \n",
    "            # Run NMS\n",
    "            keep = torchvision.ops.nms(boxes_t, scores_t, IOU_THRESHOLD)\n",
    "            final_indices = keep.cpu().numpy()\n",
    "\n",
    "        print(f\"    > Detections: {len(raw_boxes)} raw -> {len(final_indices)} unique\")\n",
    "\n",
    "        # --- D. SAVE RESULTS & VISUALIZE ---\n",
    "        \n",
    "        # Prepare visualization canvas\n",
    "        vis_image = orig_image.copy()\n",
    "        draw = ImageDraw.Draw(vis_image)\n",
    "        \n",
    "        coco_output[\"images\"].append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": filename,\n",
    "            \"width\": orig_w,\n",
    "            \"height\": orig_h\n",
    "        })\n",
    "\n",
    "        for idx in final_indices:\n",
    "            x0, y0, x1, y1 = raw_boxes[idx]\n",
    "            score = raw_scores[idx]\n",
    "            m_info = raw_masks_info[idx]\n",
    "            \n",
    "            w_box = x1 - x0\n",
    "            h_box = y1 - y0\n",
    "\n",
    "            # 1. Get Polygons (Memory Efficient)\n",
    "            # Threshold mask to binary\n",
    "            local_mask_bin = (m_info[\"mask\"] > 0.5).astype(np.uint8)\n",
    "            \n",
    "            polys = mask_to_global_polygon(\n",
    "                local_mask_bin, \n",
    "                m_info[\"x_off\"], m_info[\"y_off\"], \n",
    "                m_info[\"scale_x\"], m_info[\"scale_y\"]\n",
    "            )\n",
    "            \n",
    "            if not polys: continue\n",
    "\n",
    "            # 2. Visualize\n",
    "            # Box (Green)\n",
    "            draw.rectangle([x0, y0, x1, y1], outline=\"#00FF00\", width=3)\n",
    "            # Mask Outline (Yellow)\n",
    "            for poly in polys:\n",
    "                draw.polygon(poly, outline=\"#FFFF00\", fill=None, width=2)\n",
    "            \n",
    "            # 3. Add to COCO JSON\n",
    "            # Calc approximate area\n",
    "            local_area = np.sum(local_mask_bin)\n",
    "            global_area = local_area * m_info[\"scale_x\"] * m_info[\"scale_y\"]\n",
    "\n",
    "            coco_output[\"annotations\"].append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"file_name\": filename,\n",
    "                \"category_id\": CATEGORY_ID,\n",
    "                \"bbox\": [float(x0), float(y0), float(w_box), float(h_box)],\n",
    "                \"segmentation\": polys,\n",
    "                \"area\": float(global_area),\n",
    "                \"iscrowd\": 0,\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "        # Save Image\n",
    "        vis_image.save(os.path.join(output_img_dir, filename))\n",
    "        \n",
    "        # Cleanup Memory\n",
    "        del vis_image, draw, raw_boxes, raw_scores, raw_masks_info\n",
    "        gc.collect()\n",
    "        img_id += 1\n",
    "\n",
    "    # Save Dataset JSON\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "    print(f\"Saved JSON to {output_json}\")\n",
    "\n",
    "print(\"\\nAll processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3458002f-b41f-4b7f-99a9-2e86c6a0084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: kill: No such process\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kill -9 2956834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1badd6-9a2e-41fb-9079-f273616f998b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sam3env)",
   "language": "python",
   "name": "sam3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
