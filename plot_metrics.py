#!/usr/bin/env python3
"""
SAM3 Metrics Visualization Script

This script reads the metrics JSON files generated by metrics-mAP-cpu.py
and creates various plots for analysis.

Usage:
    python plot_metrics.py                          # Plot all datasets
    python plot_metrics.py --dataset cao2022        # Plot single dataset
    python plot_metrics.py --compare                # Compare all datasets
    python plot_metrics.py --output ./plots         # Save plots to folder
"""

import os
import json
import argparse
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from pathlib import Path

# ==========================
# CONFIGURATION
# ==========================
ROOT_DATASET = "./flatbug-dataset"
METRICS_FILENAME = "sam3_metrics_results.json"
SUMMARY_FILENAME = "sam3_metrics_summary.json"

# Plot styling
plt.style.use('seaborn-v0_8-whitegrid' if 'seaborn-v0_8-whitegrid' in plt.style.available else 'seaborn-whitegrid' if 'seaborn-whitegrid' in plt.style.available else 'ggplot')
COLORS = {
    'bbox': '#2E86AB',  # Blue
    'segm': '#A23B72',  # Magenta
    'small': '#F18F01',  # Orange
    'medium': '#C73E1D',  # Red
    'large': '#3B1F2B',  # Dark
}


def load_dataset_metrics(dataset_path):
    """Load metrics JSON for a single dataset."""
    metrics_path = os.path.join(dataset_path, METRICS_FILENAME)
    if os.path.exists(metrics_path):
        with open(metrics_path) as f:
            return json.load(f)
    return None


def load_all_metrics(root_path):
    """Load metrics for all datasets."""
    all_metrics = {}
    for dataset_name in sorted(os.listdir(root_path)):
        dataset_path = os.path.join(root_path, dataset_name)
        if os.path.isdir(dataset_path):
            metrics = load_dataset_metrics(dataset_path)
            if metrics:
                all_metrics[dataset_name] = metrics
    return all_metrics


def plot_precision_recall_curve(metrics, dataset_name, output_dir=None, show=True):
    """
    Plot Precision-Recall curves at different IoU thresholds.
    
    Creates two subplots: one for bbox and one for segmentation.
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    for ax, eval_type in zip(axes, ['bbox', 'segm']):
        eval_metrics = metrics[f'{eval_type}_metrics']
        pr_curves = eval_metrics['precision_recall_curves']
        
        # Plot curves at key IoU thresholds
        key_ious = ['IoU_0.50', 'IoU_0.75', 'IoU_0.95']
        colors = plt.cm.viridis(np.linspace(0, 0.8, len(key_ious)))
        
        for (iou_key, color) in zip(key_ious, colors):
            if iou_key in pr_curves:
                curve = pr_curves[iou_key]
                recall = curve['recall']
                precision = curve['precision']
                ap = curve['AP']
                
                if len(precision) > 0:
                    ax.plot(recall, precision, label=f'{iou_key} (AP={ap:.3f})', 
                           color=color, linewidth=2)
        
        ax.set_xlabel('Recall', fontsize=12)
        ax.set_ylabel('Precision', fontsize=12)
        ax.set_title(f'{eval_type.upper()} Precision-Recall Curve', fontsize=14)
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1])
        ax.legend(loc='lower left')
        ax.grid(True, alpha=0.3)
    
    plt.suptitle(f'Precision-Recall Curves - {dataset_name}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    if output_dir:
        save_path = os.path.join(output_dir, f'{dataset_name}_pr_curves.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved: {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()


def plot_iou_threshold_analysis(metrics, dataset_name, output_dir=None, show=True):
    """
    Plot AP vs IoU threshold to show how performance degrades with stricter IoU.
    """
    fig, ax = plt.subplots(figsize=(10, 6))
    
    for eval_type, color in [('bbox', COLORS['bbox']), ('segm', COLORS['segm'])]:
        eval_metrics = metrics[f'{eval_type}_metrics']
        iou_data = eval_metrics['ap_per_iou_threshold']
        
        iou_thresholds = iou_data['iou_thresholds']
        ap_values = iou_data['AP_values']
        
        ax.plot(iou_thresholds, ap_values, marker='o', label=eval_type.upper(), 
                color=color, linewidth=2, markersize=8)
        
        # Fill area under curve
        ax.fill_between(iou_thresholds, ap_values, alpha=0.2, color=color)
    
    ax.set_xlabel('IoU Threshold', fontsize=12)
    ax.set_ylabel('Average Precision (AP)', fontsize=12)
    ax.set_title(f'AP vs IoU Threshold - {dataset_name}', fontsize=14, fontweight='bold')
    ax.set_xlim([0.5, 0.95])
    ax.set_ylim([0, 1])
    ax.legend(loc='upper right')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if output_dir:
        save_path = os.path.join(output_dir, f'{dataset_name}_iou_analysis.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved: {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()


def plot_size_analysis(metrics, dataset_name, output_dir=None, show=True):
    """
    Plot performance breakdown by object size (small, medium, large).
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    sizes = ['small', 'medium', 'large']
    x = np.arange(len(sizes))
    width = 0.35
    
    for ax, metric_name in zip(axes, ['AP', 'AR']):
        bbox_values = [metrics['bbox_metrics']['size_analysis'][s][metric_name] for s in sizes]
        segm_values = [metrics['segm_metrics']['size_analysis'][s][metric_name] for s in sizes]
        
        bars1 = ax.bar(x - width/2, bbox_values, width, label='BBox', color=COLORS['bbox'])
        bars2 = ax.bar(x + width/2, segm_values, width, label='Segm', color=COLORS['segm'])
        
        ax.set_xlabel('Object Size', fontsize=12)
        ax.set_ylabel(metric_name, fontsize=12)
        ax.set_title(f'{metric_name} by Object Size', fontsize=14)
        ax.set_xticks(x)
        ax.set_xticklabels(['Small', 'Medium', 'Large'])
        ax.set_ylim([0, 1])
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for bar in bars1 + bars2:
            height = bar.get_height()
            if height > 0:
                ax.annotate(f'{height:.2f}',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3), textcoords="offset points",
                           ha='center', va='bottom', fontsize=9)
    
    plt.suptitle(f'Size-Based Performance Analysis - {dataset_name}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    if output_dir:
        save_path = os.path.join(output_dir, f'{dataset_name}_size_analysis.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved: {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()


def plot_bbox_vs_segm_comparison(metrics, dataset_name, output_dir=None, show=True):
    """
    Compare bounding box vs segmentation metrics.
    """
    fig, ax = plt.subplots(figsize=(10, 6))
    
    metric_names = ['mAP', 'AP50', 'AP75', 'AR_maxDet100', 'AP_small', 'AP_medium', 'AP_large']
    display_names = ['mAP', 'AP50', 'AP75', 'AR@100', 'AP(S)', 'AP(M)', 'AP(L)']
    
    x = np.arange(len(metric_names))
    width = 0.35
    
    bbox_values = [metrics['bbox_metrics']['summary'][m] for m in metric_names]
    segm_values = [metrics['segm_metrics']['summary'][m] for m in metric_names]
    
    bars1 = ax.bar(x - width/2, bbox_values, width, label='BBox', color=COLORS['bbox'])
    bars2 = ax.bar(x + width/2, segm_values, width, label='Segm', color=COLORS['segm'])
    
    ax.set_xlabel('Metric', fontsize=12)
    ax.set_ylabel('Value', fontsize=12)
    ax.set_title(f'BBox vs Segmentation Metrics - {dataset_name}', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(display_names, rotation=45, ha='right')
    ax.set_ylim([0, 1])
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    if output_dir:
        save_path = os.path.join(output_dir, f'{dataset_name}_bbox_vs_segm.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved: {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()


def plot_all_datasets_comparison(all_metrics, output_dir=None, show=True):
    """
    Create comparison plots across all datasets.
    """
    datasets = list(all_metrics.keys())
    n_datasets = len(datasets)
    
    if n_datasets == 0:
        print("No metrics found!")
        return
    
    # 1. mAP comparison bar chart
    fig, axes = plt.subplots(2, 1, figsize=(max(14, n_datasets * 0.8), 10))
    
    x = np.arange(n_datasets)
    width = 0.35
    
    # Plot mAP
    bbox_map = [all_metrics[d]['bbox_metrics']['summary']['mAP'] for d in datasets]
    segm_map = [all_metrics[d]['segm_metrics']['summary']['mAP'] for d in datasets]
    
    axes[0].bar(x - width/2, bbox_map, width, label='BBox', color=COLORS['bbox'])
    axes[0].bar(x + width/2, segm_map, width, label='Segm', color=COLORS['segm'])
    axes[0].set_ylabel('mAP (IoU=0.50:0.95)', fontsize=12)
    axes[0].set_title('mAP Comparison Across Datasets', fontsize=14, fontweight='bold')
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(datasets, rotation=45, ha='right')
    axes[0].set_ylim([0, 1])
    axes[0].legend()
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # Plot AP50
    bbox_ap50 = [all_metrics[d]['bbox_metrics']['summary']['AP50'] for d in datasets]
    segm_ap50 = [all_metrics[d]['segm_metrics']['summary']['AP50'] for d in datasets]
    
    axes[1].bar(x - width/2, bbox_ap50, width, label='BBox', color=COLORS['bbox'])
    axes[1].bar(x + width/2, segm_ap50, width, label='Segm', color=COLORS['segm'])
    axes[1].set_ylabel('AP50 (IoU=0.50)', fontsize=12)
    axes[1].set_title('AP50 Comparison Across Datasets', fontsize=14, fontweight='bold')
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(datasets, rotation=45, ha='right')
    axes[1].set_ylim([0, 1])
    axes[1].legend()
    axes[1].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    if output_dir:
        save_path = os.path.join(output_dir, 'all_datasets_comparison.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved: {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()
    
    # 2. Heatmap of all metrics
    fig, ax = plt.subplots(figsize=(max(12, n_datasets * 0.6), 8))
    
    metric_names = ['mAP', 'AP50', 'AP75', 'AR_maxDet100']
    
    # Create matrix for heatmap (datasets x metrics)
    # Interleave bbox and segm
    matrix = []
    row_labels = []
    for d in datasets:
        bbox_row = [all_metrics[d]['bbox_metrics']['summary'][m] for m in metric_names]
        segm_row = [all_metrics[d]['segm_metrics']['summary'][m] for m in metric_names]
        matrix.append(bbox_row)
        matrix.append(segm_row)
        row_labels.append(f'{d} (bbox)')
        row_labels.append(f'{d} (segm)')
    
    matrix = np.array(matrix)
    
    im = ax.imshow(matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)
    
    ax.set_xticks(np.arange(len(metric_names)))
    ax.set_yticks(np.arange(len(row_labels)))
    ax.set_xticklabels(metric_names)
    ax.set_yticklabels(row_labels)
    
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    
    # Add text annotations
    for i in range(len(row_labels)):
        for j in range(len(metric_names)):
            text = ax.text(j, i, f'{matrix[i, j]:.2f}',
                          ha="center", va="center", color="black", fontsize=8)
    
    ax.set_title('Metrics Heatmap Across All Datasets', fontsize=14, fontweight='bold')
    plt.colorbar(im, ax=ax, label='Metric Value')
    plt.tight_layout()
    
    if output_dir:
        save_path = os.path.join(output_dir, 'all_datasets_heatmap.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved: {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()


def plot_dataset_summary(metrics, dataset_name, output_dir=None, show=True):
    """
    Create a comprehensive summary figure for a single dataset.
    """
    fig = plt.figure(figsize=(16, 12))
    
    # Create grid
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    # 1. Summary metrics table (top-left)
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.axis('off')
    
    summary_text = f"""Dataset: {dataset_name}
    
Images: {metrics['dataset_info']['num_images']}
GT Annotations: {metrics['dataset_info']['num_gt_annotations']}
Predictions: {metrics['dataset_info']['num_pred_annotations']}

BBOX METRICS:
  mAP:  {metrics['bbox_metrics']['summary']['mAP']:.4f}
  AP50: {metrics['bbox_metrics']['summary']['AP50']:.4f}
  AR:   {metrics['bbox_metrics']['summary']['AR_maxDet100']:.4f}

SEGM METRICS:
  mAP:  {metrics['segm_metrics']['summary']['mAP']:.4f}
  AP50: {metrics['segm_metrics']['summary']['AP50']:.4f}
  AR:   {metrics['segm_metrics']['summary']['AR_maxDet100']:.4f}
"""
    ax1.text(0.1, 0.95, summary_text, transform=ax1.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    ax1.set_title('Summary', fontsize=12, fontweight='bold')
    
    # 2. BBox PR Curve (top-middle)
    ax2 = fig.add_subplot(gs[0, 1])
    pr_curves = metrics['bbox_metrics']['precision_recall_curves']
    for iou_key in ['IoU_0.50', 'IoU_0.75']:
        if iou_key in pr_curves:
            curve = pr_curves[iou_key]
            if len(curve['precision']) > 0:
                ax2.plot(curve['recall'], curve['precision'], 
                        label=f'{iou_key} (AP={curve["AP"]:.3f})', linewidth=2)
    ax2.set_xlabel('Recall')
    ax2.set_ylabel('Precision')
    ax2.set_title('BBox PR Curves', fontsize=12, fontweight='bold')
    ax2.set_xlim([0, 1])
    ax2.set_ylim([0, 1])
    ax2.legend(loc='lower left', fontsize=8)
    ax2.grid(True, alpha=0.3)
    
    # 3. Segm PR Curve (top-right)
    ax3 = fig.add_subplot(gs[0, 2])
    pr_curves = metrics['segm_metrics']['precision_recall_curves']
    for iou_key in ['IoU_0.50', 'IoU_0.75']:
        if iou_key in pr_curves:
            curve = pr_curves[iou_key]
            if len(curve['precision']) > 0:
                ax3.plot(curve['recall'], curve['precision'], 
                        label=f'{iou_key} (AP={curve["AP"]:.3f})', linewidth=2)
    ax3.set_xlabel('Recall')
    ax3.set_ylabel('Precision')
    ax3.set_title('Segm PR Curves', fontsize=12, fontweight='bold')
    ax3.set_xlim([0, 1])
    ax3.set_ylim([0, 1])
    ax3.legend(loc='lower left', fontsize=8)
    ax3.grid(True, alpha=0.3)
    
    # 4. IoU threshold analysis (middle row, spanning 2 columns)
    ax4 = fig.add_subplot(gs[1, :2])
    for eval_type, color in [('bbox', COLORS['bbox']), ('segm', COLORS['segm'])]:
        iou_data = metrics[f'{eval_type}_metrics']['ap_per_iou_threshold']
        ax4.plot(iou_data['iou_thresholds'], iou_data['AP_values'], 
                marker='o', label=eval_type.upper(), color=color, linewidth=2)
        ax4.fill_between(iou_data['iou_thresholds'], iou_data['AP_values'], 
                        alpha=0.2, color=color)
    ax4.set_xlabel('IoU Threshold')
    ax4.set_ylabel('Average Precision')
    ax4.set_title('AP vs IoU Threshold', fontsize=12, fontweight='bold')
    ax4.set_xlim([0.5, 0.95])
    ax4.set_ylim([0, 1])
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. Size analysis (middle row, right)
    ax5 = fig.add_subplot(gs[1, 2])
    sizes = ['small', 'medium', 'large']
    x = np.arange(len(sizes))
    width = 0.35
    bbox_ap = [metrics['bbox_metrics']['size_analysis'][s]['AP'] for s in sizes]
    segm_ap = [metrics['segm_metrics']['size_analysis'][s]['AP'] for s in sizes]
    ax5.bar(x - width/2, bbox_ap, width, label='BBox', color=COLORS['bbox'])
    ax5.bar(x + width/2, segm_ap, width, label='Segm', color=COLORS['segm'])
    ax5.set_xticks(x)
    ax5.set_xticklabels(['Small', 'Medium', 'Large'])
    ax5.set_ylabel('AP')
    ax5.set_title('AP by Object Size', fontsize=12, fontweight='bold')
    ax5.set_ylim([0, 1])
    ax5.legend()
    ax5.grid(True, alpha=0.3, axis='y')
    
    # 6. Full metrics comparison (bottom row)
    ax6 = fig.add_subplot(gs[2, :])
    metric_names = ['mAP', 'AP50', 'AP75', 'AR_maxDet1', 'AR_maxDet10', 'AR_maxDet100', 
                   'AP_small', 'AP_medium', 'AP_large', 'AR_small', 'AR_medium', 'AR_large']
    display_names = ['mAP', 'AP50', 'AP75', 'AR@1', 'AR@10', 'AR@100', 
                    'AP(S)', 'AP(M)', 'AP(L)', 'AR(S)', 'AR(M)', 'AR(L)']
    x = np.arange(len(metric_names))
    width = 0.35
    bbox_values = [metrics['bbox_metrics']['summary'][m] for m in metric_names]
    segm_values = [metrics['segm_metrics']['summary'][m] for m in metric_names]
    ax6.bar(x - width/2, bbox_values, width, label='BBox', color=COLORS['bbox'])
    ax6.bar(x + width/2, segm_values, width, label='Segm', color=COLORS['segm'])
    ax6.set_xticks(x)
    ax6.set_xticklabels(display_names, rotation=45, ha='right')
    ax6.set_ylabel('Value')
    ax6.set_title('All COCO Metrics Comparison', fontsize=12, fontweight='bold')
    ax6.set_ylim([0, 1])
    ax6.legend()
    ax6.grid(True, alpha=0.3, axis='y')
    
    plt.suptitle(f'SAM3 Evaluation Summary - {dataset_name}', fontsize=16, fontweight='bold')
    
    if output_dir:
        save_path = os.path.join(output_dir, f'{dataset_name}_summary.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Saved: {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()


def main():
    parser = argparse.ArgumentParser(description='Plot SAM3 evaluation metrics')
    parser.add_argument('--dataset', type=str, help='Specific dataset to plot')
    parser.add_argument('--compare', action='store_true', help='Compare all datasets')
    parser.add_argument('--output', type=str, help='Output directory for plots')
    parser.add_argument('--no-show', action='store_true', help='Do not display plots (save only)')
    parser.add_argument('--root', type=str, default=ROOT_DATASET, help='Root dataset directory')
    args = parser.parse_args()
    
    show = not args.no_show
    output_dir = args.output
    
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    if args.dataset:
        # Plot single dataset
        dataset_path = os.path.join(args.root, args.dataset)
        metrics = load_dataset_metrics(dataset_path)
        
        if metrics is None:
            print(f"No metrics found for dataset: {args.dataset}")
            return
        
        print(f"Plotting metrics for: {args.dataset}")
        plot_dataset_summary(metrics, args.dataset, output_dir, show)
        plot_precision_recall_curve(metrics, args.dataset, output_dir, show)
        plot_iou_threshold_analysis(metrics, args.dataset, output_dir, show)
        plot_size_analysis(metrics, args.dataset, output_dir, show)
        plot_bbox_vs_segm_comparison(metrics, args.dataset, output_dir, show)
        
    elif args.compare:
        # Compare all datasets
        all_metrics = load_all_metrics(args.root)
        if not all_metrics:
            print("No metrics found!")
            return
        
        print(f"Comparing {len(all_metrics)} datasets")
        plot_all_datasets_comparison(all_metrics, output_dir, show)
        
    else:
        # Default: create summary for all datasets
        all_metrics = load_all_metrics(args.root)
        if not all_metrics:
            print("No metrics found!")
            return
        
        print(f"Creating summaries for {len(all_metrics)} datasets")
        
        # Comparison plot
        plot_all_datasets_comparison(all_metrics, output_dir, show)
        
        # Individual summaries
        for dataset_name, metrics in all_metrics.items():
            print(f"  Processing: {dataset_name}")
            dataset_output = os.path.join(output_dir, dataset_name) if output_dir else None
            if dataset_output:
                os.makedirs(dataset_output, exist_ok=True)
            plot_dataset_summary(metrics, dataset_name, dataset_output, show=False)


if __name__ == '__main__':
    main()
