{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47e1bab-ea6f-4e50-b68c-de8f62e8a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Evaluating DATASET: NHM-beetles-crops\n",
      "==================================================\n",
      "❌ Missing GT or SAM3 file in NHM-beetles-crops, skipping.\n",
      "\n",
      "==================================================\n",
      "     OVERALL BOUNDING-BOX EVALUATION METRICS\n",
      "==================================================\n",
      "TP = 0\n",
      "FP = 0\n",
      "FN = 0\n",
      "-----------------------------------------------\n",
      "Precision = 0.0000\n",
      "Recall    = 0.0000\n",
      "F1 Score  = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==========================================\n",
    "# CONFIG\n",
    "# ==========================================\n",
    "root_dataset = \"./flatbug-dataset\"\n",
    "\n",
    "datasets_to_eval = {\n",
    "    #\"alus\",\n",
    "    #\"bioscan\",\n",
    "    #\"diversityscanner\",\n",
    "    \"nhm-beetles-crops\",\n",
    "    #\"artaxor\",\n",
    "    #\"collembolai\",\n",
    "    #\"gernat2018\",\n",
    "    #\"cao2022\",\n",
    "    #\"sittinger2023\",\n",
    "    #\"amarathunga2022\",\n",
    "    #\"biodiscover-arm\",\n",
    "}\n",
    "\n",
    "IOU_THRESHOLD = 0.5   # Standard IoU threshold for BB evaluation\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# IoU FUNCTION\n",
    "# ==========================================\n",
    "def iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two COCO bboxes: [x, y, w, h].\"\"\"\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "\n",
    "    xa = max(x1, x2)\n",
    "    ya = max(y1, y2)\n",
    "    xb = min(x1 + w1, x2 + w2)\n",
    "    yb = min(y1 + h1, y2 + h2)\n",
    "\n",
    "    inter = max(0, xb - xa) * max(0, yb - ya)\n",
    "    union = w1*h1 + w2*h2 - inter\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# GLOBAL COUNTS\n",
    "# ==========================================\n",
    "TP_global = 0\n",
    "FP_global = 0\n",
    "FN_global = 0\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PROCESS EACH DATASET\n",
    "# ==========================================\n",
    "for dataset_name in sorted(os.listdir(root_dataset)):\n",
    "\n",
    "    if dataset_name.lower() not in datasets_to_eval:\n",
    "        continue\n",
    "\n",
    "    dataset_path = os.path.join(root_dataset, dataset_name)\n",
    "\n",
    "    print(\"\\n==================================================\")\n",
    "    print(f\" Evaluating DATASET: {dataset_name}\")\n",
    "    print(\"==================================================\")\n",
    "\n",
    "    gt_path = os.path.join(dataset_path, \"instances_default.json\")\n",
    "    pred_path = os.path.join(dataset_path, \"sam3_results_BB.json\")\n",
    "\n",
    "    if not os.path.isfile(gt_path) or not os.path.isfile(pred_path):\n",
    "        print(f\"❌ Missing GT or SAM3 file in {dataset_name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    gt = json.load(open(gt_path))\n",
    "    pred = json.load(open(pred_path))\n",
    "\n",
    "    # Organize GT by image\n",
    "    gt_by_image = defaultdict(list)\n",
    "    for ann in gt[\"annotations\"]:\n",
    "        gt_by_image[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    # Organize predictions by image\n",
    "    pred_by_image = defaultdict(list)\n",
    "    for ann in pred[\"annotations\"]:\n",
    "        pred_by_image[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    # Dataset-level counts\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    # ==========================================\n",
    "    # PER-IMAGE EVALUATION\n",
    "    # ==========================================\n",
    "    for img_id in gt_by_image.keys():\n",
    "\n",
    "        gt_objs = gt_by_image[img_id]\n",
    "        pred_objs = pred_by_image.get(img_id, [])\n",
    "\n",
    "        matched_gt = set()  # indices of GT objects matched\n",
    "\n",
    "        pred_objs = sorted(pred_objs, key=lambda x: x.get(\"score\", 1.0), reverse=True)\n",
    "\n",
    "        for p in pred_objs:\n",
    "            best_iou = 0\n",
    "            best_gt = None\n",
    "\n",
    "            for idx, g in enumerate(gt_objs):\n",
    "                if idx in matched_gt:\n",
    "                    continue\n",
    "\n",
    "                ## if p[\"category_id\"] != g[\"category_id\"]:\n",
    "                ##   continue\n",
    "\n",
    "                iou_val = iou(p[\"bbox\"], g[\"bbox\"])\n",
    "\n",
    "                if iou_val > best_iou:\n",
    "                    best_iou = iou_val\n",
    "                    best_gt = idx\n",
    "\n",
    "            if best_iou >= IOU_THRESHOLD:\n",
    "                TP += 1\n",
    "                matched_gt.add(best_gt)\n",
    "            else:\n",
    "                FP += 1\n",
    "\n",
    "        FN += len(gt_objs) - len(matched_gt)\n",
    "\n",
    "    # ==========================================\n",
    "    # DATASET METRICS\n",
    "    # ==========================================\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(\"\\n--- Results for dataset:\", dataset_name, \"---\")\n",
    "    print(f\"TP = {TP}\")\n",
    "    print(f\"FP = {FP}\")\n",
    "    print(f\"FN = {FN}\")\n",
    "    print(f\"Precision = {precision:.4f}\")\n",
    "    print(f\"Recall    = {recall:.4f}\")\n",
    "    print(f\"F1 Score  = {f1:.4f}\")\n",
    "\n",
    "    # Add to global totals\n",
    "    TP_global += TP\n",
    "    FP_global += FP\n",
    "    FN_global += FN\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# GLOBAL METRICS (ALL DATASETS)\n",
    "# ==========================================\n",
    "print(\"\\n==================================================\")\n",
    "print(\"     OVERALL BOUNDING-BOX EVALUATION METRICS\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "precision_global = TP_global / (TP_global + FP_global) if TP_global + FP_global > 0 else 0\n",
    "recall_global = TP_global / (TP_global + FN_global) if TP_global + FN_global > 0 else 0\n",
    "f1_global = 2 * precision_global * recall_global / (precision_global + recall_global) if precision_global + recall_global > 0 else 0\n",
    "\n",
    "print(f\"TP = {TP_global}\")\n",
    "print(f\"FP = {FP_global}\")\n",
    "print(f\"FN = {FN_global}\")\n",
    "print(\"-----------------------------------------------\")\n",
    "print(f\"Precision = {precision_global:.4f}\")\n",
    "print(f\"Recall    = {recall_global:.4f}\")\n",
    "print(f\"F1 Score  = {f1_global:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be6d64-ff24-4de8-ab31-94e6dc2d32c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping folder: .ipynb_checkpoints\n",
      "Skipping folder: .~lock.metadata.csv#\n",
      "Skipping folder: ALUS\n",
      "Skipping folder: AMI-traps\n",
      "Skipping folder: AMT\n",
      "Skipping folder: ArTaxOr\n",
      "Skipping folder: BIOSCAN\n",
      "Skipping folder: CollembolAI\n",
      "Skipping folder: DIRT\n",
      "Skipping folder: Diopsis\n",
      "Skipping folder: DiversityScanner\n",
      "Skipping folder: Mothitor\n",
      "Skipping folder: NHM-beetles-crops\n",
      "Skipping folder: PeMaToEuroPep\n",
      "Skipping folder: abram2023\n",
      "Skipping folder: amarathunga2022\n",
      "Skipping folder: anTraX\n",
      "Skipping folder: biodiscover-arm\n",
      "Skipping folder: cao2022\n",
      "Skipping folder: gernat2018\n",
      "Skipping folder: metadata.csv\n",
      "\n",
      "======================\n",
      "EVALUATING DATASET: pinoy2023\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "\n",
    "# metrics using IOU of seg masks\n",
    "\n",
    "# ==========================\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "root_dataset = \"./flatbug-dataset\"\n",
    "\n",
    "datasets_to_eval = {\n",
    "    #\"nhm-beetles-crops\",\n",
    "    #\"cao2022\",\n",
    "    #\"gernat2018\",\n",
    "    #\"sittinger2023\",\n",
    "    #\"amarathunga2022\",\n",
    "    #\"biodiscover-arm\",\n",
    "    #\"mothitor\",\n",
    "    #\"dirt\",\n",
    "    #\"diopsis\",\n",
    "    #\"ami-traps\",\n",
    "    #\"amt\",\n",
    "    #\"pematoeuropep\",\n",
    "    #\"abram2023\",\n",
    "    #\"antrax\",\n",
    "    \"pinoy2023\",\n",
    "    \"sticky-pi\",\n",
    "    \"ubc-pitfall-traps\",\n",
    "    #todo\n",
    "    #\"alus\",\n",
    "    #\"bioscan\",\n",
    "    #\"diversityscanner\",\n",
    "    #\"artaxor\",\n",
    "    #\"collembolai\",\n",
    "    #\"Ubc-scanned-sticky-cards\",\n",
    "\n",
    "    }\n",
    "\n",
    "IOU_THRESHOLD = 0.5  # for segmentation matching\n",
    "\n",
    "# ==========================\n",
    "# UTILITIES\n",
    "# ==========================\n",
    "\n",
    "def polygons_to_mask(polygons, height, width):\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    for poly in polygons:\n",
    "        if not poly:\n",
    "            continue\n",
    "        try:\n",
    "            pts = np.array(poly, dtype=np.int32).reshape(-1, 2)\n",
    "            cv2.fillPoly(mask, [pts], 1)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return mask\n",
    "\n",
    "\n",
    "def seg_to_mask(segmentation, height, width):\n",
    "    \"\"\"Convert COCO segmentation (polygons or RLE) to binary mask.\"\"\"\n",
    "    if segmentation is None:\n",
    "        return None\n",
    "    if isinstance(segmentation, list):\n",
    "        if len(segmentation) == 0:\n",
    "            return np.zeros((height, width), dtype=np.uint8)\n",
    "        if all(isinstance(x, (list, tuple)) for x in segmentation):\n",
    "            return polygons_to_mask(segmentation, height, width)\n",
    "        return None\n",
    "    if isinstance(segmentation, dict):\n",
    "        try:\n",
    "            from pycocotools import mask as mask_utils\n",
    "            return mask_utils.decode(segmentation).astype(np.uint8)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def mask_iou(mask1, mask2):\n",
    "    inter = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    return float(inter) / float(union) if union > 0 else 0.0\n",
    "# ==========================\n",
    "# GLOBAL COUNTERS\n",
    "# ==========================\n",
    "TP_global = 0\n",
    "FP_global = 0\n",
    "FN_global = 0\n",
    "# ==========================\n",
    "# DATASET EVALUATION\n",
    "# =========================\n",
    "for dataset_name in sorted(os.listdir(root_dataset)):\n",
    "    if dataset_name.lower() not in datasets_to_eval:\n",
    "        print(f\"Skipping folder: {dataset_name}\")\n",
    "        continue\n",
    "\n",
    "    dataset_path = os.path.join(root_dataset, dataset_name)\n",
    "    gt_file = os.path.join(dataset_path, \"instances_default.json\")\n",
    "    ##pred_file = os.path.join(dataset_path, \"sam3_results.json\")\n",
    "    pred_file = os.path.join(dataset_path, \"sam3_results_pyramid_v2.json\")\n",
    "\n",
    "    if not os.path.isfile(gt_file) or not os.path.isfile(pred_file):\n",
    "        print(f\"❌ Missing GT or SAM3 predictions in {dataset_name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n======================\\nEVALUATING DATASET: {dataset_name}\\n======================\")\n",
    "\n",
    "    gt = json.load(open(gt_file))\n",
    "    pred = json.load(open(pred_file))\n",
    "\n",
    "    # map annotations by file_name instead of image_id\n",
    "    gt_by_file = defaultdict(list)\n",
    "    gt_image_sizes = {}\n",
    "    for im in gt.get(\"images\", []):\n",
    "        gt_image_sizes[im[\"file_name\"]] = (im[\"height\"], im[\"width\"])\n",
    "    for ann in gt[\"annotations\"]:\n",
    "        file_name = next((im[\"file_name\"] for im in gt[\"images\"] if im[\"id\"] == ann[\"image_id\"]), None)\n",
    "        if file_name:\n",
    "            gt_by_file[file_name].append(ann)\n",
    "\n",
    "    pred_by_file = defaultdict(list)\n",
    "    for ann in pred[\"annotations\"]:\n",
    "        file_name = ann.get(\"file_name\")\n",
    "        if file_name:\n",
    "            pred_by_file[file_name].append(ann)\n",
    "\n",
    "    # dataset counters\n",
    "    TP = FP = FN = 0\n",
    "    skipped_pred_count = 0\n",
    "    missing_size_count = 0\n",
    "\n",
    "    # per-image evaluation\n",
    "\n",
    "    for file_name, gt_objs in gt_by_file.items():\n",
    "        if file_name not in gt_image_sizes:\n",
    "            missing_size_count += 1\n",
    "            print(f\"WARNING: {file_name} missing size info, skipping\")\n",
    "            continue\n",
    "\n",
    "        H, W = gt_image_sizes[file_name]\n",
    "        pred_objs = pred_by_file.get(file_name, [])\n",
    "\n",
    "        gt_masks = [seg_to_mask(g.get(\"segmentation\"), H, W) for g in gt_objs]\n",
    "        gt_cats = [g.get(\"category_id\") for g in gt_objs]\n",
    "\n",
    "        pred_masks = []\n",
    "        pred_cats = []\n",
    "        pred_scores = []\n",
    "        for p in pred_objs:\n",
    "            mask = seg_to_mask(p.get(\"segmentation\"), H, W)\n",
    "            if mask is None:\n",
    "                skipped_pred_count += 1\n",
    "            pred_masks.append(mask)\n",
    "            pred_cats.append(p.get(\"category_id\"))\n",
    "            pred_scores.append(p.get(\"score\", 1.0))\n",
    "\n",
    "\n",
    "        # match predictions to GT\n",
    "        matched_gt = set()\n",
    "        order = sorted(range(len(pred_objs)), key=lambda i: pred_scores[i] if pred_masks[i] is not None else 0.0, reverse=True)\n",
    "        for pi in order:\n",
    "            pmask = pred_masks[pi]\n",
    "            if pmask is None:\n",
    "                continue\n",
    "            pcat = pred_cats[pi]\n",
    "\n",
    "            best_iou = 0.0\n",
    "            best_gi = None\n",
    "\n",
    "            for gi, (gmask, gcat) in enumerate(zip(gt_masks, gt_cats)):\n",
    "                if gi in matched_gt or gmask is None:\n",
    "                    continue\n",
    "                ##if pcat != gcat:\n",
    "                ##    continue\n",
    "                iou = mask_iou(pmask, gmask)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gi = gi\n",
    "\n",
    "            if best_iou >= IOU_THRESHOLD and best_gi is not None:\n",
    "                TP += 1\n",
    "                matched_gt.add(best_gi)\n",
    "            else:\n",
    "                FP += 1\n",
    "\n",
    "        FN += sum(1 for gi in range(len(gt_objs)) if gi not in matched_gt and gt_masks[gi] is not None)\n",
    "\n",
    "    # dataset metrics\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "\n",
    "    print(f\"TP={TP} FP={FP} FN={FN} Precision={precision:.4f} Recall={recall:.4f} F1={f1:.4f}\")\n",
    "    print(f\"Skipped pred masks: {skipped_pred_count}, Missing image sizes: {missing_size_count}\")\n",
    "\n",
    "    # update global totals\n",
    "    TP_global += TP\n",
    "    FP_global += FP\n",
    "    FN_global += FN\n",
    "\n",
    "# ==========================\n",
    "# Overall metrics\n",
    "# ==========================\n",
    "precision_global = TP_global / (TP_global + FP_global) if TP_global + FP_global > 0 else 0\n",
    "recall_global = TP_global / (TP_global + FN_global) if TP_global + FN_global > 0 else 0\n",
    "f1_global = 2 * precision_global * recall_global / (precision_global + recall_global) if precision_global + recall_global > 0 else 0\n",
    "\n",
    "\n",
    "print(\"\\n======================\")\n",
    "print(\"OVERALL SEGMENTATION EVALUATION\")\n",
    "print(f\"TP={TP_global} FP={FP_global} FN={FN_global}\")\n",
    "print(f\"Precision={precision_global:.4f} Recall={recall_global:.4f} F1={f1_global:.4f}\")\n",
    "print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f327f1-1852-4cbf-960f-2b6036fbe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f733a1-3581-4c0c-af1c-aa35ddbd7596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 14 14:07:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN Xp                Off |   00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8              9W /  250W |       2MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA TITAN Xp                Off |   00000000:83:00.0 Off |                  N/A |\n",
      "| 23%   26C    P8              8W /  250W |       2MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e17d40-6a01-49cb-8133-ee587c5a56dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0          -     -      -      -      -      -      -      -    -              \n",
      "# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command \n",
      "# Idx           #    C/G      %      %      %      %      %      %    name \n",
      "    1          -     -      -      -      -      -      -      -    -              \n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi pmon -c 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec63dd-8b93-4908-a9a7-70c135f364f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -9 python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sam3env)",
   "language": "python",
   "name": "sam3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
